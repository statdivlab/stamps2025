---
title: "Differential Abundance Lab"
author: "Amy Willis, Sarah Teichman, David Clausen"
date: "2025-07-21"
output: html_document
editor_options: 
  chunk_output_type: console
---

In this lab we'll explore a dataset published by Wirbel et al. (2019). (https://www.nature.com/articles/s41591-019-0406-6). 

This is a meta-analysis of case-control studies, meaning that Wirbel et al. collected raw sequencing data from studies other researchers conducted and re-analyzed it (in this case, they also collected some new data of their own).

Wirbel et al. published two pieces of data we'll focus on today:

1) metadata giving demographics and other information about participants

2) a mOTU (metagenomic OTU) table

Although this lab works with mOTUs generated from shotgun data, `radEmu` will also work with OTUs generated from amplicon data, or other types of categories (genes, etc.) that we observe as abundance measurements from high-throughput sequencing. This is true for some of the other methods, although `ALDEx2` and `DESeq2` both will only work when data are integer-valued. 

In this analysis, we're most interested in comparing microbial abundance in cases diagnosed with colorectal cancer to abundances in controls (without this diagnosis).

First let's load libraries we'll need. 

```{r, include=FALSE}
library(tidyverse)
# install radEmu (only if you don't already have it installed)
if (!("radEmu" %in% row.names(installed.packages()))) {
  devtools::install_github("https://github.com/statdivlab/radEmu")
}
library(radEmu)
```

## Exploring the data

We can start by loading the metadata table.

```{r}
data("wirbel_sample")
```

Use `dim()` and `head()` to check out the size and contents of this metadata table. 

```{r}

```

Let's see how many observations we have among cases ("CRC") and controls ("CTR")

```{r}
wirbel_sample %>%
  group_by(Group) %>%
  summarize(n = length(Group))
```

We have data from studies in five different countries. How much from each study, you ask? You can find out using the same syntax as in the previous code chunk, replacing "Group" with the correct variable name.

```{r}
names(wirbel_sample) # look at variable names

# your code here 
```

Now let's load the mOTU table.

```{r}
data("wirbel_otu")
```

Use `dim()` to check out the size of this dataset. Often `head()` produces too much output for a count table, so instead subset `wirbel_otu` to the first five samples and the first five columns to get a sense of what it looks like (hint: the syntax `data[rows you want, columns you want]` is useful here). 

```{r}

```

Based on the dimensions of the metadata and mOTU data, how are these data tables related? 

Let's save these mOTU names in a vector.

```{r}
mOTU_names <- colnames(wirbel_otu)
```

## The plan 

`radEmu` is a package that can be used to estimate log fold differences in the abundance of microbial taxa between levels of a covariate. In this analysis, the covariate that we are primarily interested in is whether a sample is from a case of colorectal cancer or a control. 

## Preparing our data 

First, we will make control ("CTR") the reference level:

```{r}
wirbel_sample$Group <- factor(wirbel_sample$Group, levels = c("CTR","CRC"))
```

We would generally fit a model using all of our samples, but for this lab, we are only going to consider data from a case-control study from China.

```{r}
ch_study_obs <- which(wirbel_sample$Country %in% c("CHI"))
```

Finally, we want to confirm that all samples have at least one non-zero count across the mOTUs we've chosen and that all mOTUs have at least one non-zero count across the samples we've chosen. It doesn't make sense to use samples that had no taxa observed! Again, this wouldn't happen at home -- it's just an artifact of restricting our taxon set (from the code chunk above). 

```{r}
abundances_subset <- wirbel_otu[ch_study_obs, ]
sum(rowSums(abundances_subset) == 0) # zero samples have a count sum of 0
sum(colSums(abundances_subset) == 0) # 87 mOTUs have count sums of 0

categories_to_rm <- which(colSums(abundances_subset) == 0)
abundances_subset <- abundances_subset[, -categories_to_rm]
sum(colSums(abundances_subset) == 0) ## good
```

Ok, let's fit the model!

## Fitting a model

The function that we use to fit our model is called `emuFit`. It can accept your data in various forms, and here we will show how to use it with data frames as input. You can also input `phyloseq` objects!

One version of inputs to `emuFit` are

- `formula`: This is a formula telling radEmu what predictors to use. We are
             using Group, which is an indicator for case (CRC) vs control (CTR).
- `data`: A data frame containing information on our predictors. Recall that
          here we're only looking observations from the Chinese study.
- `Y`: A matrix or data frame containing our observed abundance data
       (e.g., counts or depth measurements). The rows give the observations
       (samples), and the columns give the categories (taxa/mOTUs). Here we are
       only considering the observations from the Chinese study.

There's one more important argument to know about: 

- `run_score_tests`: A logical value denoting whether or not to run score tests.
                    Score tests are awesome in their error rate control (including
                    with small sample sizes; though of course larger sample sizes
                    always give better power), but require refitting the model, so
                    can require some compute time. For now we will not run them.

Let's now fit the model! For now, we're only going to include our covariate of interest, "Group", to more efficiently demonstrate this package. However, you could replace this with whatever regression model you are interested in, with multiple covariates. 

When running this on the RStudio server, this takes about 2.5 minutes. Feel free to run it below (get up, stretch, grab some more coffee), or skip that command and load in the results in the line below. 

```{r, eval = FALSE}
ch_fit <- emuFit(formula = ~ Group,
                 data = wirbel_sample[ch_study_obs, ],
                 Y = abundances_subset,
                 run_score_tests = FALSE)

# uncomment the next line to load in the results
# ch_fit <- readRDS("differential-abundance/ch_fit.rds")
```

Let's check out what this object looks like!

```{r}
ch_fit
```

The way to access estimated coefficients and confidence intervals from the model is with `ch_fit$coef`. Let's look at one, then save the estimates in a vector. 

```{r}
ch_fit$coef[64, ] # look at the 64th taxon, Fusobacterium nucleatum s. vincentii [ref_mOTU_v2_0754] 
radEmu_est <- ch_fit$coef$estimate
```

Let's interpret these results! Here's three equivalent interpretations:

- We estimate that the abundance of *Fusobacterium nucleatum s. vincentii [ref_mOTU_v2_0754]* in metagenomes from cases with CRC is $e^{1.051} = 2.9$ times greater than non-CRC controls, when compared to the typical fold-differences in the abundance of taxa across these groups. (Yep -- that's a *ratio* of *ratios*.)

- We estimate that the log-fold difference in the abundance of *Fusobacterium nucleatum s. vincentii [ref_mOTU_v2_0754]* in metagenomes from cases with and without CRC is $1.051$ greater than the typical log-fold difference in the abundance of taxa across these groups. (That's a *difference* in *differences*.)

- Under the assumumption that most taxa do not differ in abundance between cases with and without CRC, we estimate that the abundance of *Fusobacterium nucleatum s. vincentii [ref_mOTU_v2_0754]* in metagenomes from cases with CRC is $e^{1.051} = 2.9$ times greater than non-CRC controls. 

By default `radEmu` compares the log fold difference in one taxon to the *typical* (approximately the median) log fold difference in all taxa. However, `radEmu` can estimate other types of parameters. For example, we can compare log fold differences to the log fold difference for a specific (or reference) taxon.  

What reference taxon should we use? That's up to you. If there is a specific taxon that you're interested in comparing to, you can use that one. Or you could choose 

- the mOTU that has the highest abundance across all samples
- the mOTU that is present in the highest number of samples
- the mOTU that has the lowest variance in raw counts across all samples
- etc. 

We'll give you an example below, but feel free to replace this with a mOTU that you choose (and flag down the TAs if you need help with the code to find the mOTU that you care about)! 

We'll find the mOTU that is present in the highest proportion of samples from this cohort, and use this as a reference mOTU.

```{r}
num_present <- colSums(abundances_subset > 0)
ref_num <- which.max(num_present)
ref_num
```

Now we will refit `radEmu` with the new parameter that we are estimating. Again, feel free to run yourself (if you could use a little more coffee), or skip this command and load in the results in the line afterwards. 

```{r, eval = FALSE}
ch_fit_ref <- emuFit(formula = ~ Group,
                     data = wirbel_sample[ch_study_obs, ],
                     Y = abundances_subset,
                     run_score_tests = FALSE,
                     # set our "constraint" i.e. choose our reference mOTU
                     constraint_fn = ref_num)

# uncomment the next line to load in the results
# ch_fit_ref <- readRDS("differential-abundance/ch_fit_ref.rds")

ch_fit_ref$coef[64, ]
```

How we will interpret our parameter now? Fill in the following:

We estimate that the  _______________________________ in *Fusobacterium nucleatum s. vincentii [ref_mOTU_v2_0754]* compared to _______________________________ is  _______________________________. 

We can compare the estimates from these two runs of `radEmu`.

```{r}
data.frame(typical_est = ch_fit$coef$estimate,
           reference_est = ch_fit_ref$coef$estimate) %>%
  ggplot(aes(x = typical_est, y = reference_est)) + 
  geom_point() + 
  geom_abline(aes(intercept = 0, slope = 1), color = "red") + 
  labs(x = "Estimate compared to typical mOTU",
       y = "Estimate compared to reference mOTU")
```

The $x = y$ line is shown in red. What do you notice about the relationship between the estimates from the first run of `radEmu` and the estimates from the second run of `radEmu`? 

What is the numerical difference between the estimate of any mOTU in `ch_fit` and the estimate of the same mOTU in `ch_fit_ref`?

```{r}

```

Let's switch back to the model that we fit first, in which we compare each log fold difference to the typical log fold difference across all mOTUs, and look closer at our results. 

```{r}
ch_fit$coef %>% 
  arrange(estimate) %>%
  mutate(order = 1:n()) %>%
  ggplot(aes(x = order, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = lower, ymax = upper), alpha = 0.3) + 
  labs(x = "", y = "Estimate (with 95% confidence interval)") + 
  ggtitle("Log fold-difference estimates") + 
  theme(plot.title = element_text(hjust = 0.5))
```

Here we can see the distribution of log fold-difference estimates from our model, as well as 95% confidence intervals. 

Let's look closer at some of these mOTUs. Assume that you are particularly interested in *Clostridium* mOTUs, as you've read studies in which certain species of *Clostridium* were associated with colorectal cancer. Specifically, let's look at two mOTUs: *Clostridium citroniae [ref_mOTU_v2_4882]* and *Clostridium clostridioforme [ref_mOTU_v2_0979]*. 

```{r}
taxa_names_to_test <- c("Clostridium citroniae [ref_mOTU_v2_4882]",
                        "Clostridium clostridioforme [ref_mOTU_v2_0979]")
ch_fit$coef %>% filter(category %in% taxa_names_to_test) 
exp(2.511)
exp(0.856)
```

The estimates suggest that these taxa are both enriched in colorectal cancer cases and the confidence interval for these two estimates do not include zero -- but (!!!) the kind of confidence interval that is returned by default by `emuFit()` is not extremely reliable when counts are very skewed or sample size is small-to-moderate.

To investigate further, let's run robust score tests for these two mOTUs, which is more reliable in these settings (but also takes more time because apparently we can't have nice things). 

To set up this test, we can again run `emuFit`, giving it the fitted values that it has
already found:

- `formula`, `data` and `Y` are as before
- `ch_fit` is our previous fitted object (the output of `emuFit`)
- `test_kj` a data frame listing the indices of the parameters (in `ch_fit$B`) that we want
to test. Below we show how to identify these, but `j = 87` is *C. clostridioforme* and `j = 224` is *C. citroniae*.

This should take about a minute to run on the RStudio server. 

```{r}
taxa_to_test <- which(colnames(abundances_subset) %in%
                        taxa_names_to_test)
ch_fit$B %>% rownames
covariate_to_test <- which("GroupCRC" == ch_fit$B %>% rownames)
two_robust_score_tests <- emuFit(formula = ~ Group,
                                 data = wirbel_sample[ch_study_obs, ],
                                 fitted_model = ch_fit,
                                 refit = FALSE, # we already have ch_fit, don't recompute this 
                                 test_kj = data.frame(k = covariate_to_test,
                                                      j = taxa_to_test),
                                 Y = abundances_subset,
                                 verbose = TRUE, # have it tell you when it runs score tests
                                 compute_cis = FALSE) # don't need to re-compute confidence intervals
```

Let's take a look at the test output.

```{r}
two_robust_score_tests$coef[taxa_to_test, c("covariate", "category", "estimate", "pval")]
```

The *C. clostridioforme* mOTU has a robust score test p-value of 0.191 and the *C. citroniae* mOTU has a robust score test p-value of 0.002. This means that we have stronger statistical evidence that the fold-difference in abundance of *C. citroniae* between colorectal cancer cases and controls is different than the typical fold-difference in this dataset, and weaker evidence for the analogous *C. clostridioforme* fold-difference. 

Now it's your turn. Explore the results in `ch_fit$coef` and choose one mOTU that you would like to investigate further. Run a robust score test for that mOTU. 

```{r, eval = FALSE}
# explore ch_fit$coef object 

taxon_to_test <- which(colnames(abundances_subset) %in%
                        c("your mOTU here"))
covariate_to_test <- which("GroupCRC" == ch_fit$B %>% rownames)
your_robust_score_tests <- emuFit(formula = ~ Group,
                                  data = wirbel_sample[ch_study_obs, ],
                                  fitted_model = ch_fit,
                                  refit = FALSE,
                                  test_kj = data.frame(k = covariate_to_test,
                                                      j = taxon_to_test),
                                  Y = abundances_subset,
                                  compute_cis = FALSE,
                                  verbose = TRUE)
```

What did you learn about the mOTU that you chose? Does the p-value align with what you expected from the estimate and confidence interval? 

In many data analyses, we'd like to run reliable tests for all taxa that we have measured. We could run robust score tests for every taxon in this analysis, but it will take ~1.5 days to run all tests serially (although they can easily be run in parallel on a computing cluster!). Therefore we do not suggest evaluating it here (note the argument in the code chunk `eval = FALSE`).

```{r, eval = FALSE}
# We do not suggest that you run this now! 
test_all <- emuFit(formula = ~ Group,
                   data = wirbel_sample[ch_study_obs, ],
                   fitted_model = ch_fit,
                   refit = FALSE,
                   Y = abundances_subset,
                   run_score_tests = TRUE)
```

However, we ran these robust score tests and saved the results, which you can load in here. 

```{r}
radEmu_score_res <- readRDS("differential-abundance/rad_res.rds")
radEmu_pvals <- radEmu_score_res$pval
summary(radEmu_pvals)
```

I ran these score tests in parallel on a computing cluster. Let's look at how long these tests took.

```{r}
summary(radEmu_score_res$time / 60)
```

The median test took 0.2 minutes, the mean test took 2.4 minutes, and the maximum test took 35.5 minutes. Running in parallel, this analysis took me ~40 minutes. If you'd like an example of running these score tests in parallel on a computing cluster, check in with Sarah! 

## FastEmu

While running these $758$ tests is quite reasonable with the appropriate computing resources, there are some analyses with many thousands of taxa for which running `radEmu` robust score tests could be computationally prohibitive. Luckily, we can approximate the robust score tests with a companion method to `radEmu`, `fastEmu`!

```{r, include=FALSE}
if (!("fastEmu" %in% row.names(installed.packages()))) {
  devtools::install_github("https://github.com/statdivlab/fastEmu")
}
library(fastEmu)
```

We will start by comparing the parameters estimated by `fastEmu` to `radEmu`. `fastEmu` works by fitting a simpler model than the one used by `radEmu`, while targeting a very similar parameter. In order to define this simpler model, we need a parameter that relies on a subset of taxa, as opposed to one defined relative to the "typical log fold-difference across all taxa." In `fastEmu`, we instead compare to the "typical log fold-difference across taxa in a reference set." Ideally, the typical log fold-difference in this reference set would be quite similar to the typical log fold-difference in the full analysis. 

In `fastEmu`, the user can either decide which taxa to include in this reference set, or a "data-driven" reference set can be determined. This "data-driven" reference set is a small set of taxa that have estimated log fold-differences that are closest to the typical (approximate median) estimated log fold-difference from `radEmu`. Below, we can use `fastEmuFit()` to determine a "data-driven" reference set. 

```{r}
fastMod <- fastEmuFit(formula = ~ Group,
                      data = wirbel_sample[ch_study_obs, ],
                      fitted_model = ch_fit,
                      refit = FALSE,
                      Y = abundances_subset,
                      run_score_tests = FALSE,
                      reference_set = "data_driven")
fastMod$reference_set_names
fastMod$constraint_diff
```

Above, we can see the reference sets chosen for column of the design matrix (here we have the intercept and the "Group" covariate). The `constraint_diff` object tells us the difference between the typical log fold-difference across all taxa and the typical log fold-difference across the reference set, for each design matrix column. These differences represent a very small shift in all parameter estimates from `radEmu` to `fastEmu`. 

Now we can run robust score tests with `fastEmu`. 

```{r}
taxa_to_test <- which(colnames(abundances_subset) %in%
                        c(taxa_names_to_test,
                          "add your mOTU here"))

fastEmu_test <- fastEmuFit(formula = ~ Group,
                           data = wirbel_sample[ch_study_obs, ],
                           Y = abundances_subset,
                           test_kj = data.frame(k = 2, j = taxa_to_test),
                           compute_cis = FALSE,
                           fitted_model = fastMod,
                           verbose = TRUE)
```

These score tests ran very quickly! Let's check out the results. 

```{r}
fastEmu_test$coef[taxa_to_test, ]
```

Let's compare these `fastEmu` results from our earlier `radEmu` results. 

```{r}
two_robust_score_tests$coef[taxa_to_test, ]
```

Looking between these, we can see that (as described above), the estimates are slightly different because we've slightly adjusted the parameters that we're estimating. The p-values are not the same, but are quite similar and lead to similar conclusions. We don't expect them to be exactly the same, because they rely on a slightly different model, however both `radEmu` and `fastEmu` provide valid tests of nearly the same parameters. 

We also ran `fastEmu` robust score tests for all mOTUs in this analysis. 

```{r}
fastEmu_score_res <- readRDS("differential-abundance/fast_res.rds")
fastEmu_pvals <- fastEmu_score_res$pval
cor(radEmu_pvals, fastEmu_pvals) # very high correlation between radEmu and fastEmu p-values 
```

Let's check out the timing information for these tests. 

```{r}
summary(radEmu_score_res$time / fastEmu_score_res$time)
sum(fastEmu_score_res$time / (60))
```

The speed increase that we get from using `fastEmu` instead of `radEmu` ranges from a minimum of 2.4 times faster, to a median of 15 times faster, to a maximum of 600 times faster. If you ran these tests serially (assuming similar run times on your laptop to our cluster), you could run this analysis in 43 minutes. You could do it during your lunch break, without needing additional computational resources! 

We recommend using `fastEmu` over `radEmu` when running differential abundance analyses for very large data sets. That said, you do need to choose a reference set to use `fastEmu`, which makes the interpretation of parameter estimates less intuitive than for `radEmu`. If you find that `radEmu` is taking too long for your differential abundance analysis, consider trying `fastEmu`!  
## Fitting other models

While we like `radEmu` and `fastEmu`, they aren't the only differential abundance testing methods that you can use. Let's see how these results relate to differential abundance methods implemented in other software packages. Note that each software will be testing different hypotheses with different models and different estimators.

### ALDEx2

`ALDEx2` is built to analyze RNA-seq data but can be used for microbiome data as well. 

```{r, include=FALSE}
if (!require("BiocManager", quietly = TRUE))
  install.packages("BiocManager")
if (!("ALDEx2" %in% row.names(installed.packages()))) {
  BiocManager::install("ALDEx2")
}
library(ALDEx2)
# More information can be found in vignettes and in the papers cited here:
citation("ALDEx2")
```

To start, we need to get our data into a form that `ALDEx2` will accept. 

```{r}
covariate <- wirbel_sample$Group[ch_study_obs]
# aldex doesn't like a factor, so we will convert the covariate to TRUE/FALSE
covariate_boolean <- ifelse(covariate == "CRC", TRUE, FALSE)
X <- cbind(1, covariate_boolean)
# swap rows and columns (ALDEx2 requires samples as columns)
abundances_subset_transpose <- t(abundances_subset)
```

Now we're ready to run `ALDEx2`. This will take a few minutes. A great time to grab a snack! 

```{r, message = FALSE}
set.seed(714)
aldex_clr <- aldex.clr(abundances_subset_transpose, X, mc.samples = 128, denom = "all", verbose = FALSE)
aldex_res <- aldex.glm(clr = aldex_clr, X)
aldex_est <- aldex_res$`covariate_boolean:Est` / log(2) # divide by log(2) to transform from log base 2 to log base e
aldex_pvals <- aldex_res$`covariate_boolean:pval`
```

### ANCOM-BC2

`ANCOM-BC2` is a differential abundance method built specifically for microbiome data. 

```{r, include=FALSE}
if (!("ANCOMBC" %in% row.names(installed.packages()))) {
  BiocManager::install("ANCOMBC")
}
library(ANCOMBC)
# More information can be found in vignettes and in the papers cited here:
citation("ANCOMBC")
```

`ANCOM-BC2` requires certain data formats. One option is a `TreeSummarizedExperiment` object.

```{r, include=FALSE}
if (!("TreeSummarizedExperiment" %in% row.names(installed.packages()))) {
  BiocManager::install("TreeSummarizedExperiment")
}
library(TreeSummarizedExperiment)
```

We can start by making a `TreeSummarizedExperiment` object with our data.

```{r}
tse_data <- TreeSummarizedExperiment(assays = list("counts" = t(abundances_subset)),
                                     colData = data.frame(Group = covariate))
```

Now we can run `ANCOM-BC2`. This will take another few minutes. Maybe you have an email or two to respond to. 

```{r, warning = FALSE}
ancom_res <- ancombc2(data = tse_data,
                      assay_name = "counts",
                      fix_formula = "Group",
                      prv_cut = 0)
ancom_est <- ancom_res$res$lfc_GroupCRC
ancom_pvals <- ancom_res$res$p_GroupCRC
table(is.na(ancom_est))
```

When we look at `ancom_est` we can see that we have $120$ estimates are recorded as `NA`. This is because `ANCOM-BC2` cannot compute estimates (nor perform inferences) for mOTUs that have all-zero counts in one of the covariate levels. This is one disadvantage of `ANCOM-BC2` not shared by `radEmu`.  

### DESeq2

`DESeq2` is another method that was built for RNA-seq data, but is commonly used for microbiome differential abundance analyses. Note that even the developer of DESeq2, Mike Love, recommends **against** DESeq2 for microbiome data because the assumptions aren't satistfied, and has previously endorsed StatDivLab tools for microbiome data. Thanks, Mike!! <3 

```{r, include=FALSE}
if (!("DESeq2" %in% row.names(installed.packages()))) {
  BiocManager::install("DESeq2")
}
library(DESeq2)
# More information can be found in vignettes and in the papers cited here:
citation("DESeq2")
```

Now, we can run `DESeq2`. 

```{r, message = FALSE}
dds <- DESeqDataSetFromMatrix(countData = abundances_subset_transpose,
                              colData = wirbel_sample[ch_study_obs, ],
                              design = ~ Group)
deseq_res <- DESeq(dds, sfType = "poscounts")
deseq_results <- results(deseq_res)
deseq_est <- deseq_results$log2FoldChange / log(2) 
# divide the log base 2 fold change estimates by log 2 so that they will be on the same scale as the log base e parameters
deseq_pvals <- deseq_results$pvalue
```

### LinDA

`LinDA` is a newer microbiome-focused method that uses a log-ratio transformation after including pseudocounts. 

```{r, include=FALSE}
if (!("MicrobiomeStat" %in% row.names(installed.packages()))) {
  install.packages("MicrobiomeStat")
}
library(MicrobiomeStat)
# More information can be found in vignettes and in the papers cited here:
citation("MicrobiomeStat")
```

Now, we can run `LinDA`. 

```{r, message = FALSE}
linda_res <- linda(feature.dat = abundances_subset_transpose, meta.dat = wirbel_sample[ch_study_obs, ],
                   formula = "~ Group", feature.dat.type = "count", is.winsor = FALSE)
linda_est <- linda_res$output$GroupCRC$log2FoldChange / log(2)
# another log base 2 estimate that we want to convert to log base e
linda_pvals <- linda_res$output$GroupCRC$pvalue
```

## Comparing across methods

Now that we've run each of these methods, let's compare the estimates and p-values. We'll start with the estimates. 

```{r}
data.frame(radEmu_est, aldex_est, ancom_est, deseq_est, linda_est) %>%
  cor(use = "complete.obs")
```

What do you note about the relationship between estimates? Does this surprise you? Why or why not?

Next, we can compare the p-values. We'll use the package `GGally` to visually compare each set of p-values. 

```{r}
if (!("GGally" %in% row.names(installed.packages()))) {
  install.packages("GGally")
}
library(GGally)
pval_df <- data.frame(mOTU = names(abundances_subset),
  radEmu_pvals, aldex_pvals, ancom_pvals, deseq_pvals, linda_pvals)
ggpairs(
     pval_df %>% dplyr::select(-mOTU),
     upper = list(continuous = wrap("points", size = 0.5, alpha = 0.5)),
     lower = list(continuous = wrap("points", size = 0.5, alpha = 0.5))
) # Click "Zoom" in the plot window to see these plots larger 
```

What is one thing that you notice about the distribution of p-values for one of the methods (look at the density plots on the diagonal, a peak means that there are a more p-values near that value and a valley means that there are fewer p-values near that value)?

For example, one thing that I notice is that ANCOM-BC2 has a large peak near 0, which means that is has many mOTUs with p-values near 0. 

What is one thing that you learn about how the p-values from two methods vary together. 

For example, I see that ANCOM-BC2 has many mOTUs with p-values that are near 0 or near 1, and those mOTUs have radEmu p-values that are spread out pretty evenly from 0 to 1.

While there are many observations we can make about p-value distributions and how the relationships between p-values vary across these methods, a major takeaway is that while these methods have more similarity between estimates, they produce quite different p-values for the same mOTUs. 

Finally, we can compute q-values (to control the false discovery rate across the $758$ tests we ran) and see which mOTUs each method finds to be significant. We'll use the `qvalue` package for this. 

```{r, include = FALSE}
if (!("qvalue" %in% row.names(installed.packages()))) {
  BiocManager::install("qvalue")
}
library(qvalue)
# More information can be found in vignettes and in the papers cited here:
citation("qvalue")
```

```{r}
qval_df <- pval_df %>%
  mutate(ancom_sens = ancom_res$res$passed_ss_GroupCRC,
         ancom_pvals_with_ss = ifelse(ancom_sens == TRUE, 
                                      ancom_pvals,
                                      NA)) %>%
        # ancom-bc2 runs a sensitivity analysis for each mOTU
        # and recommends ignoring results for taxa that don't 
        # pass this sensitivity analysis - the StatDivLab are 
        # skeptical of this approach, but we'll follow it here 
  mutate(radEmu_q = qvalue::qvalue(radEmu_pvals)$qvalues,
         # we add pi0 = 1 because aldex p-values don't range from 0-1 as 'qvalue'
         # expects, they instead range from 0 to ~0.94
         aldex_q = qvalue::qvalue(aldex_pvals, pi0 = 1)$qvalues,
         ancom_q = qvalue::qvalue(ancom_pvals_with_ss)$qvalues,
         deseq_q = qvalue::qvalue(deseq_pvals)$qvalues,
         linda_q = qvalue::qvalue(linda_pvals)$qvalues) %>% 
  dplyr::select(-contains("pvals"), -ancom_sens) %>%
  pivot_longer(cols = 2:6, names_to = "method", values_to = "qval") 
qval_df %>%
  group_by(method) %>%
  summarise(min(qval, na.rm = TRUE))
```

Here we can see that the minimum q-value from `radEmu` is $0.07$, the minimum q-value from `ALDEx2` is $0.08$, and the minimum q-values for `ANCOM-BC2`, `DESeq2`, and `LinDA` are less than $0.001$. 

Interpret the meaning of the minimum q-value from `radEmu` using your knowledge of the definition of a q-value. 

```{r}
qval_df %>%
  group_by(method) %>%
  summarise(sum(qval < 0.1, na.rm = TRUE))
```

Here we can see that when we use a false discovery rate threshold of $10\%$, `ALDEx2` is the most conservative (makes the fewest discoveries) and `ANCOM-BC2` is the least conservative (makes the most discoveries), even after we consider the results of the sensitivity analysis. 

```{r}
# see how many discoveries overlap across methods 
qval_disc <- qval_df %>%
  pivot_wider(names_from = method, values_from = qval) %>%
  dplyr::select(-mOTU) %>%
  mutate(across(everything(), ~ . <= 0.1))
methods <- colnames(qval_disc)
shared_matrix <- matrix(0, nrow = length(methods), ncol = length(methods),
                        dimnames = list(methods, methods))
for (i in methods) {
  for (j in methods) {
    shared_matrix[i, j] <- sum(qval_disc[[i]] & qval_disc[[j]], na.rm = TRUE)
  }
}
shared_matrix
```

In this matrix, the diagonals tell us the number of discoveries for each method, and the off-diagonals tell us how many discoveries are made in common by the row method and column method. What is one thing that you learn from this matrix?

In StatDivLab simulations, we found that in some settings with large sample sizes, many taxa, and sparse high-variance data, `ANCOM-BC2`, `DESeq2`, and `LinDA` fail to control Type I error. Therefore, we would be somewhat hesitant to claim "significant! FDR<0.1!" here for the taxa found by these methods. `radEmu` and `ALDEx2` do control Type I error in our simulation settings, although `ALDEx2` is quite conservative (finds few signals). Since we've extensively tested `radEmu` and find that it controls Type I error in the simulation settings we are interested in and has higher power than `ALDEx2`, we will look at results from `radEmu`.

```{r}
qval_df %>% 
  filter(method == "radEmu_q") %>%
  filter(qval <= 0.1) %>%
  mutate(category = mOTU) %>%
  left_join(ch_fit$coef) %>%
  dplyr::select(category, estimate, qval) %>%
  arrange(qval) %>%
  print(n = 23)
```

Is there anything interesting in these results? Consider estimates as well as small q-values! 

## Conclusions 

In this lab, we've compared several differential abundance methods: `radEmu`, `fastEmu`, `ALDEx2`, `ANCOM-BC2`, and `DESeq2`. We've seen that while there are strong correlations between the log fold difference estimates from each of these methods, there are weaker correlations between the p-values and different conclusions in terms of the most differentially abundant mOTUs. Each of these methods (and the many differential abundance methods that we didn't consider here) have pros and cons. An incomplete list includes the following: 

- `radEmu`
  - pros: has Type I error rate control in all settings that we tested, handles sparsity well
  - cons: robust score tests are slow, especially in data sets with a large number of categories (taxa, genes, etc.)
- `fastEmu`
  - pros: inherits `radEmu` pros, is much faster than `radEmu` (especially with a large number of categories)
  - cons: an approximate method (rather than an exact method) with a less intuitive target parameter, still slower than several other methods 
- `ALDEx2`
  - pros: has Type I error rate control in many situations, faster than `radEmu` to test all categories
  - cons: has lower power (conservative), handles zeroes poorly/weirdly
- `ANCOM-BC2`
  - pros: high power, faster than `radEmu` to test all categories
  - cons: fails to control Type I error rate in some situations (!), can't handle data separation (a common consequence of sparsity), relies on questionable sensitivity analysis 
- `DESeq2`
  - pros: faster than `radEmu` to test all categories 
  - cons: fails to control Type I error in some situations, tailored for RNA-seq data, not microbiome data, its own developers recommend against it for this purpose
- `LinDA`
  - pros: faster than `radEmu` to test all categories, high power in some settings
  - cons: fails to control Type I error in some situations (large sample sizes with sparse data), handles zeroes poorly/weirdly

Check out the documentation for each of these packages for more information and more practice using them. Although this example uses a single binary covariate, each of these methods can be run with more complex regression models. 